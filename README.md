# Optimizer Dynamics: SGD vs Adam

Исследовательский pet-проект, посвящённый анализу динамики оптимизаторов в нейросетях.
Фокус — не на архитектуре или SOTA-результатах, а на **механике обучения, масштабах градиентов и эффективных шагах оптимизации**.


---

## Цели проекта

- Практически разобраться в различиях между SGD и Adam
- Понять, как learning rate влияет на:
  - скорость сходимости
  - устойчивость обучения
  - эффективный шаг обновления параметров
- Научиться **диагностировать обучение через логи**, а не только по loss/accuracy

---

## Эксперименты

### SGD vs Adam (`01_sgd_vs_adam.ipynb`)

**Задача:** бинарная классификация `make_moons`  
**Модель:** MLP (2 → 128 → 1, ReLU, He init)

Логируем:
- train / validation loss и accuracy
- `grad.global`, `param.global`, `update.global`
- эффективный шаг `update / grad`
- насыщение логитов

**Выводы:**
- Adam сходится быстрее и достигает более низкого validation loss
- Для SGD `update / grad = lr` (константа)
- Для Adam эффективный шаг существенно больше и адаптивен

---

### Learning Rate Sweep (`02_lr_sweep.ipynb`)

**SGD:**
- высокая чувствительность к learning rate
- узкая область оптимальных значений (~ 0.1–0.3)
- при малых lr обучение замедляется

**Adam:**
- устойчивость на диапазоне более чем в порядок (~ 7e-4 – 1e-2)
- слабая зависимость качества от точного значения lr

**Ключевой вывод:**  
Adam адаптивно нормализует шаг обновления, что делает его значительно более робастным.

---

### Momentum (`03_momentum.ipynb`)

**Цель:** понять, как momentum меняет поведение SGD

**Наблюдения:**
- momentum увеличивает эффективный шаг SGD
- сглаживает траекторию обновлений
- приближает поведение SGD к Adam, но без координатной адаптивности

---

## Основные метрики и диагностика

Проект использует расширенное логирование:

- L2-нормы градиентов и параметров (глобально и по слоям)
- величину обновлений параметров
- эффективный шаг
- динамику логитов и насыщения

Все выводы основаны **на логах**, а не на эвристиках.

---

## Структура проекта
optimizer-dynamics/

├── data/ # генерация и масштабирование данных

├── models/ # MLP

├── optim/ # фабрика оптимизаторов

├── metrics/ # нормы градиентов, параметров и апдейтов

├── train/ # универсальный train loop

├── experiments/ # ноутбуки экспериментов

└── README.md

